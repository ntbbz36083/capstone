{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "# Loading all library\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "import boto3\n",
    "import time\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "# Change padans parameter to adjust visliazation\n",
    "pd.set_option('max_colwidth', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "# Load in aws credential\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "KEY = config.get('AWS','KEY')\n",
    "SECRET = config.get('AWS','SECRET')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['KEY']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that creates spark session\n",
    "start_time = time.time()\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    This function is used to create a spark session to work in\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "# Create spark session\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from AWS s3\n",
    "#df_spark = spark.read.csv(\"/home/workspace/ibrd-statement-of-loans-historical-data.csv\", header=True)\n",
    "\n",
    "df_spark = spark.read.csv(\"s3a://udacity-leejohn/loan/ibrd-statement-of-loans-historical-data.csv\", header=True)\n",
    "# function that will uppercase everything in the dataframe\n",
    "fields = df_spark.schema.fields\n",
    "stringFields = filter(lambda f: isinstance(f.dataType, StringType), fields)\n",
    "nonStringFields = map(lambda f: col(f.name), filter(lambda f: not isinstance(f.dataType, StringType), fields))\n",
    "stringFieldsTransformed = map(lambda f: upper(col(f.name)), stringFields) \n",
    "allFields = [*stringFieldsTransformed, *nonStringFields]\n",
    "df_new = df_spark.select(allFields)\n",
    "# rename the column name\n",
    "# Get old column names \n",
    "oldColumns = df_new.schema.names\n",
    "# Setup new column names\n",
    "newColumns  = ['End_of_Period', 'Loan_Number', 'Region', 'Country_Code', 'Country','Borrower','Guarantor_Country_Code','Guarantor','Loan_Type','Loan_Status','Interest_Rate','Currency_of_Commitment','Project_ID','Project_Name','Original_Principal_Amount','Cancelled_Amount','Undisbursed_Amount','Disbursed_Amount','Repaid_to_IBRD','Due_to_IBRD','Exchange_Adjustment','Borrowers_Obligation','Sold_3rd_Party','Repaid_3rd_Party','Due_3rd_Party','Loans_Held','First_Repayment_Date','Last_Repayment_Date','Agreement_Signing_Date','Board_Approval_Date','Effective_Date_Most_Recent','Closed_Date_Most_Recent','Last_Disbursement_Date']\n",
    "# Rename the dataframe\n",
    "df = reduce(lambda df_spark, idx: df_spark.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>End_of_Period</th>\n",
       "      <th>Loan_Number</th>\n",
       "      <th>Region</th>\n",
       "      <th>Country_Code</th>\n",
       "      <th>Country</th>\n",
       "      <th>Borrower</th>\n",
       "      <th>Guarantor_Country_Code</th>\n",
       "      <th>Guarantor</th>\n",
       "      <th>Loan_Type</th>\n",
       "      <th>Loan_Status</th>\n",
       "      <th>Interest_Rate</th>\n",
       "      <th>Currency_of_Commitment</th>\n",
       "      <th>Project_ID</th>\n",
       "      <th>Project_Name</th>\n",
       "      <th>Original_Principal_Amount</th>\n",
       "      <th>Cancelled_Amount</th>\n",
       "      <th>Undisbursed_Amount</th>\n",
       "      <th>Disbursed_Amount</th>\n",
       "      <th>Repaid_to_IBRD</th>\n",
       "      <th>Due_to_IBRD</th>\n",
       "      <th>Exchange_Adjustment</th>\n",
       "      <th>Borrowers_Obligation</th>\n",
       "      <th>Sold_3rd_Party</th>\n",
       "      <th>Repaid_3rd_Party</th>\n",
       "      <th>Due_3rd_Party</th>\n",
       "      <th>Loans_Held</th>\n",
       "      <th>First_Repayment_Date</th>\n",
       "      <th>Last_Repayment_Date</th>\n",
       "      <th>Agreement_Signing_Date</th>\n",
       "      <th>Board_Approval_Date</th>\n",
       "      <th>Effective_Date_Most_Recent</th>\n",
       "      <th>Closed_Date_Most_Recent</th>\n",
       "      <th>Last_Disbursement_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5557</td>\n",
       "      <td>31542</td>\n",
       "      <td>59135</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>26382</td>\n",
       "      <td>802089</td>\n",
       "      <td>31389</td>\n",
       "      <td>198899</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>3822</td>\n",
       "      <td>3726</td>\n",
       "      <td>10075</td>\n",
       "      <td>96</td>\n",
       "      <td>4699</td>\n",
       "      <td>842</td>\n",
       "      <td>351895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   End_of_Period  Loan_Number  Region  Country_Code  Country  Borrower  \\\n",
       "0              0            0       0             0        0      5557   \n",
       "\n",
       "   Guarantor_Country_Code  Guarantor  Loan_Type  Loan_Status  Interest_Rate  \\\n",
       "0                   31542      59135         48           48          26382   \n",
       "\n",
       "   Currency_of_Commitment  Project_ID  Project_Name  \\\n",
       "0                  802089       31389        198899   \n",
       "\n",
       "   Original_Principal_Amount  Cancelled_Amount  Undisbursed_Amount  \\\n",
       "0                         48                48                  48   \n",
       "\n",
       "   Disbursed_Amount  Repaid_to_IBRD  Due_to_IBRD  Exchange_Adjustment  \\\n",
       "0                48              48           48                   48   \n",
       "\n",
       "   Borrowers_Obligation  Sold_3rd_Party  Repaid_3rd_Party  Due_3rd_Party  \\\n",
       "0                    48              48                48             48   \n",
       "\n",
       "   Loans_Held  First_Repayment_Date  Last_Repayment_Date  \\\n",
       "0          48                  3822                 3726   \n",
       "\n",
       "   Agreement_Signing_Date  Board_Approval_Date  Effective_Date_Most_Recent  \\\n",
       "0                   10075                   96                        4699   \n",
       "\n",
       "   Closed_Date_Most_Recent  Last_Disbursement_Date  \n",
       "0                      842                  351895  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is to display the number of null value of each column, before we do any cleaning, you will see null data count for each column with below command\n",
    "df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since below 4 columns have too much null values(average 50% null values) and are not necessary, so we drop them\n",
    "df = df.drop('Last_Disbursement_Date')\n",
    "df = df.drop('Currency_of_Commitment')\n",
    "df = df.drop('Borrower')\n",
    "df = df.drop('Project_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit the loan_number to make it 9 digits\n",
    "# Find records with 9 digits Loan_Number, which is the correct data format we are looking for\n",
    "df_good = df.filter(length(col(\"Loan_Number\")) == 9)\n",
    "\n",
    "# Find records that have less than 9 digits Loan_Number, replace them to the correct format\n",
    "df_6 = df.where(length(col(\"Loan_Number\")) == 6).withColumn(\"Loan_Number\", regexp_replace(col(\"Loan_Number\") ,  \"(\\\\w{4})(\\\\d{2})\" , \"$1000$2\" ))\n",
    "df_7 = df.where(length(col(\"Loan_Number\")) == 7).withColumn(\"Loan_Number\", regexp_replace(col(\"Loan_Number\") ,  \"(\\\\w{4})(\\\\d{3})\" , \"$100$2\" ))\n",
    "df_8 = df.where(length(col(\"Loan_Number\")) == 8).withColumn(\"Loan_Number\", regexp_replace(col(\"Loan_Number\") ,  \"(\\\\w{4})(\\\\d{4})\" , \"$10$2\" ))\n",
    "\n",
    "# No records with loan_number that has less than 9 digits\n",
    "df = df_good.union(df_6).union(df_7).union(df_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if both Guarantor_Country_Code and Guarantor are empty, then it's hard to say whether they are suppose to be empty(no guarantor)\n",
    "# or they are missing values, so I just drop them.\n",
    "df = df.filter('Guarantor_Country_Code is not NULL or Guarantor is not NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each loan number, there should be one country code. Run below code, we will find there are 3 records that \n",
    "#df.select('Loan_Number','Country_Code').distinct().groupBy('Loan_Number').count().withColumnRenamed('count', 'ccount').filter('count>1').toPandas()\n",
    "#withColumnRenamed('count', 'ccount').groupBy('Loan_Number').count().filter('count>1').toPandas()\n",
    "#xx = df.select('Loan_Number','Country').distinct().groupBy('Loan_Number').count().filter('count>1').select('Loan_Number').collect()\n",
    "#xarr = [str(xx[i].Loan_Number) for i in range(len(xx))]\n",
    "#for i in xarr:\n",
    "#    print(i)\n",
    "#df.where(df.Loan_Number == 'IBRD82610').select('Country').groupBy('Country').count().toPandas()\n",
    "#df.where(df.Loan_Number == 'IBRD82550').select('Country').groupBy('Country').count().toPandas()\n",
    "#df.where(df.Loan_Number == 'IBRD82580').select('Country').groupBy('Country').count().toPandas()\n",
    "df = df.where((df.Loan_Number == 'IBRD82580') & (df.Country != 'CHINA') | (df.Loan_Number != 'IBRD82580'))\n",
    "df = df.where((df.Loan_Number == 'IBRD82550') & (df.Country != 'CHINA') | (df.Loan_Number != 'IBRD82550'))\n",
    "df = df.where((df.Loan_Number == 'IBRD82610') & (df.Country != 'INDIA') | (df.Loan_Number != 'IBRD82610'))\n",
    "# For each loan number, there should be one country code. Run below code, we will find there are 3 records that \n",
    "#df.select('Loan_Number','Country_Code').distinct().groupBy('Loan_Number').count().withColumnRenamed('count', 'ccount').filter('count>1').toPandas()\n",
    "#withColumnRenamed('count', 'ccount').groupBy('Loan_Number').count().filter('count>1').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all missing value for Project_ID column\n",
    "# Create a dict that key is loan_number and value is project_id\n",
    "x1 = df.filter('Project_ID is not NULL').select('Loan_Number','Project_ID').distinct().toPandas().set_index('Loan_Number')['Project_ID'].to_dict() \n",
    "# Create a pandas dataframe that only has the records that have missing project_id\n",
    "pdf = df.filter('Project_ID is NULL').toPandas()\n",
    "# Loop through the dataframe and replace its missing value with the loan_number\n",
    "for index,row in pdf.iterrows():\n",
    "    att = row.Loan_Number\n",
    "    if att in x1.keys():\n",
    "        row.Project_ID = x1.get(att)\n",
    "# Convert it back to a spark dataframe\n",
    "ddd = spark.createDataFrame(pdf.astype(str)).filter('Project_ID is not NULL')\n",
    "# Union it with the good records and get new dataframe\n",
    "df = df.filter('Project_ID is not NULL').union(ddd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all missing value for Guarantor and Guarantor_Country_Code column\n",
    "# Create a dict that key is loan_number and value is a array of country, country_code\n",
    "c1 = df.select('Loan_Number','Country','Country_Code').distinct().toPandas().set_index('Loan_Number').T.to_dict('list')\n",
    "# there is one more country in Guarantor, which is united kingdom, we will add it to our dict\n",
    "c1b = df.filter('Guarantor == \\'UNITED KINGDOM\\'').select('Loan_Number').distinct().collect()\n",
    "for i in c1b:\n",
    "    c1[i.Loan_Number] = ['UNITED KINGDOM','GB']\n",
    "# Loop through the dataframe and replace its missing value with the loan_number\n",
    "pdf = df.filter('Guarantor is NULL or Guarantor_Country_Code is NULL').toPandas()\n",
    "for index,row in pdf.iterrows():\n",
    "    att = row.Loan_Number\n",
    "    if att in c1.keys():\n",
    "        row.Guarantor = c1.get(att)[0]\n",
    "        row.Guarantor_Country_Code = c1.get(att)[1]\n",
    "# Convert it back to a spark dataframe\n",
    "ddd = spark.createDataFrame(pdf).filter('Guarantor is not NULL and Guarantor_Country_Code is not NULL')\n",
    "# Union it with the good records and get new dataframe\n",
    "df = df.filter('Guarantor is not NULL and Guarantor_Country_Code is not NULL').union(ddd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all missing value for Interest_Rate\n",
    "# Since for each loan number, we will find different interest rate. So to handle this case, for rach loan number, I will use mean value of all intertest rate to repalce its nul value. \n",
    "# Create a dict that key is loan_number and value is intertest rate\n",
    "i1 = df.filter('Interest_Rate is not NULL').filter('Interest_Rate != \\'None\\'').select('Loan_Number','Interest_Rate').distinct().toPandas()\n",
    "i1.Interest_Rate = i1.Interest_Rate.astype(np.float16)\n",
    "i1 = i1.groupby('Loan_Number', as_index=True).agg({\"Interest_Rate\": \"mean\"})['Interest_Rate'].to_dict()\n",
    "pdf = df.filter('Interest_Rate is NULL').toPandas()\n",
    "# Loop through the dataframe and replace its missing value with the loan_number\n",
    "for index,row in pdf.iterrows():\n",
    "    att = row.Interest_Rate\n",
    "    if att in c1.keys():\n",
    "        row.Interest_Rate = c1.get(att)\n",
    "# Convert it back to a spark dataframe\n",
    "ddd = spark.createDataFrame(pdf.astype(str)).filter('Interest_Rate is not NULL')\n",
    "# Union it with the good records and get new dataframe\n",
    "df = df.filter('Interest_Rate is not NULL').union(ddd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a function that will take a dataframe and a column and replace its null value with the same method from above cases.\n",
    "def clean(df, column):\n",
    "    x1 = df.filter('{} is not NULL'.format(column)).filter('{} != \\'None\\''.format(column)).select('Loan_Number','{}'.format(column)).distinct().toPandas().set_index('Loan_Number')['{}'.format(column)].to_dict()\n",
    "    pdf = df.filter('{} is NULL'.format(column)).toPandas()\n",
    "    for index,row in pdf.iterrows():\n",
    "        att = row.Loan_Number\n",
    "        if att in x1.keys():\n",
    "            row.column = x1.get(att)\n",
    "    ddd = spark.createDataFrame(pdf.astype(str)).filter('{} is not NULL'.format(column)).filter('{} != \\'None\\''.format(column))\n",
    "    df = df.filter('{} is not NULL'.format(column)).union(ddd)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1087.240961074829 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# use this function to repalce all missing values for the date related column.\n",
    "df = clean(df,'First_Repayment_Date')\n",
    "df = clean(df,'Agreement_Signing_Date')\n",
    "df = clean(df,'Closed_Date_Most_Recent')\n",
    "df = clean(df,'Effective_Date_Most_Recent')\n",
    "df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).toPandas()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all above we clean our data and now there's no null value in the data anymore. this process will take about 17 minutes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start from here, we will create file for fact table and dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate country dataframe \n",
    "df_country = df.select('Country_Code','Country','Region').distinct()\n",
    "newRow = spark.createDataFrame([('GB','United Kingdom','EUROPE AND CENTRAL ASIA')])\n",
    "df_country = df_country.union(newRow)\n",
    "# add a unique id Country_Id for this dataframe\n",
    "df_country = df_country.withColumn('Country_Id',row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "#df_country.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate time dataframe \n",
    "# By exploring data, we can see that for each loan number, it should only have one set of dates related column. However, we will find for some column, it has more than one combination. so i crete this function to find its more frequent value for each loan number. \n",
    "def getss(df, column):\n",
    "    # create a sql table with loan number and column\n",
    "    df.select('Loan_Number', column).createOrReplaceTempView(\"time\")\n",
    "    # run query to get the most frequent value of each category for each loan number\n",
    "    x = spark.sql(\"\"\"\n",
    "    SELECT Loan_Number, {} FROM \n",
    "    (\n",
    "    SELECT Loan_Number, {}, count(1) total_records, ROW_NUMBER() OVER (PARTITION BY Loan_Number ORDER BY count(1) desc) AS seqnum\n",
    "    FROM time \n",
    "    group by Loan_Number, {}\n",
    "    )\n",
    "    WHERE seqnum = 1\n",
    "    \"\"\".format(column, column, column))\n",
    "    return x\n",
    "x1 = getss(df, 'First_Repayment_Date')\n",
    "x2 = getss(df, 'Last_Repayment_Date')\n",
    "x3 = getss(df, 'Agreement_Signing_Date')\n",
    "x4 = getss(df, 'Board_Approval_Date')\n",
    "x5 = getss(df, 'Effective_Date_Most_Recent')\n",
    "x6 = getss(df, 'Closed_Date_Most_Recent')\n",
    "df_time = x1.join(x2, x1.Loan_Number == x2.Loan_Number).drop(x2.Loan_Number)\n",
    "df_time = df_time.join(x3, df_time.Loan_Number == x3.Loan_Number).drop(x3.Loan_Number)\n",
    "df_time = df_time.join(x4, df_time.Loan_Number == x4.Loan_Number).drop(x4.Loan_Number)\n",
    "df_time = df_time.join(x5, df_time.Loan_Number == x5.Loan_Number).drop(x5.Loan_Number)\n",
    "df_time = df_time.join(x6, df_time.Loan_Number == x6.Loan_Number).drop(x6.Loan_Number)\n",
    "df_time = df_time.withColumn('Time_Id',row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "df_time = df_time.selectExpr('Time_Id', 'Loan_Number', 'First_Repayment_Date as First_Repayment_Date_t', 'Last_Repayment_Date as Last_Repayment_Date_t', 'Agreement_Signing_Date as Agreement_Signing_Date_t','Board_Approval_Date \\\n",
    "                         as Board_Approval_Date_t','Effective_Date_Most_Recent as Effective_Date_Most_Recent_t','Closed_Date_Most_Recent as Closed_Date_Most_Recent_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Amount dataframe \n",
    "df_amount = df.select('Loan_Number','Original_Principal_Amount','Sold_3rd_Party','Repaid_3rd_Party', 'Due_3rd_Party').distinct()\n",
    "# add a unique id Amount_Id for this dataframe\n",
    "df_amount = df_amount.withColumn('Amount_Id',row_number().over(Window.orderBy(monotonically_increasing_id())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Loan_Type dataframe \n",
    "df_loan_type = df.select('Loan_Type').distinct()\n",
    "# add a unique id Loan_Type_Id for this dataframe\n",
    "df_loan_type = df_loan_type.withColumn('Loan_Type_Id',row_number().over(Window.orderBy(monotonically_increasing_id())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Loan_Status dataframe \n",
    "df_loan_status = df.select('Loan_Status').distinct()\n",
    "# add a unique id Loan_Status_Id for this dataframe\n",
    "df_loan_status = df_loan_status.withColumn('Loan_Status_Id',row_number().over(Window.orderBy(monotonically_increasing_id())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sql tables \n",
    "df_country.createOrReplaceTempView(\"country\")\n",
    "df_time.createOrReplaceTempView(\"time\")\n",
    "df_amount.createOrReplaceTempView(\"amount\")\n",
    "df_loan_type.createOrReplaceTempView(\"loan_type\")\n",
    "df_loan_status.createOrReplaceTempView(\"loan_status\")\n",
    "df.createOrReplaceTempView(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate transaction dataframe by joing log table with the other tables on loan number\n",
    "log_norm = spark.sql(\"\"\"\n",
    "    select l.Loan_Number, t.Time_Id, c.Country_Id, cc.Country_Id as Guarantor_Country_Id, lt.Loan_Type, ls.Loan_Status_Id, a.Amount_Id,\n",
    "    l.End_of_Period, l.Interest_Rate, l.Project_ID, l.Exchange_Adjustment, l.Borrowers_Obligation, l.Cancelled_Amount,\n",
    "    l.Undisbursed_Amount, l.Disbursed_Amount, l.Repaid_to_IBRD, l.Due_to_IBRD, l.Loans_Held\n",
    "    from log l \n",
    "    left join country c on l.Country_Code= c.Country_Code\n",
    "    left join country cc on l.Guarantor = cc.Country \n",
    "    left join loan_type lt on l.Loan_Type = lt.Loan_Type \n",
    "    left join loan_status ls on l.Loan_Status = ls.Loan_Status\n",
    "    left join amount a on l.Loan_Number = a.Loan_Number\n",
    "    left join time t on l.Loan_Number = t.Loan_Number\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the loan number column for the dataframes\n",
    "df_amount_final = df_amount.select('Amount_Id','Original_Principal_Amount','Sold_3rd_Party','Repaid_3rd_Party', 'Due_3rd_Party')\n",
    "df_time_final = df_time.selectExpr('Time_Id', 'First_Repayment_Date_t as First_Repayment_Date', 'Last_Repayment_Date_t as Last_Repayment_Date',\\\n",
    "                                   'Agreement_Signing_Date_t as Agreement_Signing_Date','Board_Approval_Date_t \\\n",
    "                                     as Board_Approval_Date','Effective_Date_Most_Recent_t as Effective_Date_Most_Recent','Closed_Date_Most_Recent_t as Closed_Date_Most_Recent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving files to S3, this process will take about 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = time.time()\n",
    "#df_country.write.csv(\"/home/workspace/csv/country.csv\")\n",
    "#df_time_final.write.parquet(\"s3a://udacity-leejohn/loan/time.parquet\")\n",
    "#df_loan_status.write.parquet(\"s3a://udacity-leejohn/loan/loan_status.parquet\")\n",
    "#df_loan_type.write.parquet(\"s3a://udacity-leejohn/loan/loan_type.parquet\")\n",
    "#df_amount_final.write.parquet(\"s3a://udacity-leejohn/loan/amount.parquet\")\n",
    "#log_norm.write.parquet(\"s3a://udacity-leejohn/loan/log_nor.parquet\")\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 53.77740550041199 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "df_country.write.csv(\"/home/workspace/csv/country.csv\")\n",
    "df_time_final.write.csv(\"/home/workspace/csv/time.csv\")\n",
    "df_loan_status.write.csv(\"/home/workspace/csv/loan_status.csv\")\n",
    "df_loan_type.write.csv(\"/home/workspace/csv/loan_type.csv\")\n",
    "df_amount_final.write.csv(\"/home/workspace/csv/amount.csv\")\n",
    "log_norm.write.csv(\"/home/workspace/csv/log_nor.csv\")\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run below script will create a redshift cluster and it will take about 5-10 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i 'Redshift.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-east-1\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )\n",
    "## This is to display what we have in S3 \n",
    "sampleDbBucket =  s3.Bucket(\"john-udacity-s3\")\n",
    "i = 0\n",
    "for obj in sampleDbBucket.objects.filter(Prefix=\"movie\"):\n",
    "    print(obj)\n",
    "    i += 1\n",
    "    if i > 10:\n",
    "        break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/workspace'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'shutil' is not defined\n",
      "name 'shutil' is not defined\n",
      "name 'shutil' is not defined\n",
      "name 'shutil' is not defined\n",
      "name 'shutil' is not defined\n",
      "name 'shutil' is not defined\n",
      "name 'shutil' is not defined\n"
     ]
    }
   ],
   "source": [
    "# This is used to delete output file\n",
    "def Delete_All(file_path):\n",
    "    folder = file_path\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "Delete_All('/home/workspace/csv')\n",
    "#Delete_All('data/log_data')\n",
    "#Delete_All('Output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
