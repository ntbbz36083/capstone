{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "--describe your project at a high level--\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "# Loading all library\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "import boto3\n",
    "import time\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "# Change padans parameter to adjust visliazation\n",
    "pd.set_option('max_colwidth', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "# Load in aws credential\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('csp.cfg'))\n",
    "KEY                      = config.get('AWS','AWS_ACCESS_KEY_ID')\n",
    "SECRET                   = config.get('AWS','AWS_SECRET_ACCESS_KEY')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that creates spark session\n",
    "start_time = time.time()\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    This function is used to create a spark session to work in\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "# Create spark session\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from AWS s3\n",
    "df_spark = spark.read.csv(\"s3a://john-udacity-s3/loan/ibrd-statement-of-loans-historical-data.csv\", header=True)\n",
    "# function that will uppercase everything in the dataframe\n",
    "fields = df_spark.schema.fields\n",
    "stringFields = filter(lambda f: isinstance(f.dataType, StringType), fields)\n",
    "nonStringFields = map(lambda f: col(f.name), filter(lambda f: not isinstance(f.dataType, StringType), fields))\n",
    "stringFieldsTransformed = map(lambda f: upper(col(f.name)), stringFields) \n",
    "allFields = [*stringFieldsTransformed, *nonStringFields]\n",
    "df_new = df_spark.select(allFields)\n",
    "# rename the column name\n",
    "oldColumns = df_new.schema.names\n",
    "newColumns  = ['End_of_Period', 'Loan_Number', 'Region', 'Country_Code', 'Country','Borrower','Guarantor_Country_Code','Guarantor','Loan_Type','Loan_Status','Interest_Rate','Currency_of_Commitment','Project_ID','Project_Name','Original_Principal_Amount','Cancelled_Amount','Undisbursed_Amount','Disbursed_Amount','Repaid_to_IBRD','Due_to_IBRD','Exchange_Adjustment','Borrowers_Obligation','Sold_3rd_Party','Repaid_3rd_Party','Due_3rd_Party','Loans_Held','First_Repayment_Date','Last_Repayment_Date','Agreement_Signing_Date','Board_Approval_Date','Effective_Date_Most_Recent','Closed_Date_Most_Recent','Last_Disbursement_Date']\n",
    "df = reduce(lambda df_spark, idx: df_spark.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to display the number of null value of each column\n",
    "#from pyspark.sql.functions import col,sum\n",
    "#df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.filter('Project_ID = \\'P051696\\'').distinct().toPandas()\n",
    "#df.select('Borrower','Loan_Number').groupBy('Borrower','Loan_Number').count().withColumnRenamed('count', 'ccount').groupBy('Loan_Number').count().filter('count>1').toPandas()\n",
    "#df.filter('Project_ID = \\'P051696\\'').distinct().toPandas()\n",
    "#df.filter('Project_Name is not NULL').select('Project_Name','Loan_Number').groupBy('Project_Name','Loan_Number').count().withColumnRenamed('count', 'ccount').groupBy('Loan_Number').count().filter('count>1').toPandas()\n",
    "\n",
    "# Drop null value\n",
    "df = df.drop('Last_Disbursement_Date')\n",
    "df = df.drop('Currency_of_Commitment')\n",
    "df = df.drop('Borrower')\n",
    "df = df.drop('Project_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit the loan_number to make it 9 digits\n",
    "df_good = df.filter(length(col(\"Loan_Number\")) == 9)\n",
    "df_6 = df.where(length(col(\"Loan_Number\")) == 6).withColumn(\"Loan_Number\", regexp_replace(col(\"Loan_Number\") ,  \"(\\\\w{4})(\\\\d{2})\" , \"$1000$2\" ))\n",
    "df_7 = df.where(length(col(\"Loan_Number\")) == 7).withColumn(\"Loan_Number\", regexp_replace(col(\"Loan_Number\") ,  \"(\\\\w{4})(\\\\d{3})\" , \"$100$2\" ))\n",
    "df_8 = df.where(length(col(\"Loan_Number\")) == 8).withColumn(\"Loan_Number\", regexp_replace(col(\"Loan_Number\") ,  \"(\\\\w{4})(\\\\d{4})\" , \"$10$2\" ))\n",
    "# No records with loan_number that has less than 9 digits\n",
    "df = df_good.union(df_6).union(df_7).union(df_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if both Guarantor_Country_Code and Guarantor are empty, then it's hard to say whether they are suppose to be empty(no guarantor)\n",
    "# or they are missing values, so I just drop them.\n",
    "df = df.filter('Guarantor_Country_Code is not NULL or Guarantor is not NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each loan number, there should be one country code. Run below code, we will find there are 3 records that \n",
    "#df.select('Loan_Number','Country_Code').distinct().groupBy('Loan_Number').count().withColumnRenamed('count', 'ccount').filter('count>1').toPandas()\n",
    "#withColumnRenamed('count', 'ccount').groupBy('Loan_Number').count().filter('count>1').toPandas()\n",
    "#xx = df.select('Loan_Number','Country').distinct().groupBy('Loan_Number').count().filter('count>1').select('Loan_Number').collect()\n",
    "#xarr = [str(xx[i].Loan_Number) for i in range(len(xx))]\n",
    "#for i in xarr:\n",
    "#    print(i)\n",
    "#df.where(df.Loan_Number == 'IBRD82610').select('Country').groupBy('Country').count().toPandas()\n",
    "#df.where(df.Loan_Number == 'IBRD82550').select('Country').groupBy('Country').count().toPandas()\n",
    "#df.where(df.Loan_Number == 'IBRD82580').select('Country').groupBy('Country').count().toPandas()\n",
    "df = df.where((df.Loan_Number == 'IBRD82580') & (df.Country != 'CHINA') | (df.Loan_Number != 'IBRD82580'))\n",
    "df = df.where((df.Loan_Number == 'IBRD82550') & (df.Country != 'CHINA') | (df.Loan_Number != 'IBRD82550'))\n",
    "df = df.where((df.Loan_Number == 'IBRD82610') & (df.Country != 'INDIA') | (df.Loan_Number != 'IBRD82610'))\n",
    "# For each loan number, there should be one country code. Run below code, we will find there are 3 records that \n",
    "#df.select('Loan_Number','Country_Code').distinct().groupBy('Loan_Number').count().withColumnRenamed('count', 'ccount').filter('count>1').toPandas()\n",
    "#withColumnRenamed('count', 'ccount').groupBy('Loan_Number').count().filter('count>1').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all missing value for Borrower column\n",
    "# Get distinct Loan_Number for the records that are not null\n",
    "x1 = df.filter('Project_ID is not NULL').select('Loan_Number','Project_ID').distinct().toPandas().set_index('Loan_Number')['Project_ID'].to_dict() \n",
    "# Get distinct Loan_Number for the records that are null\n",
    "pdf = df.filter('Project_ID is NULL').toPandas()\n",
    "for index,row in pdf.iterrows():\n",
    "    att = row.Loan_Number\n",
    "    if att in x1.keys():\n",
    "        row.Project_ID = x1.get(att)\n",
    "ddd = spark.createDataFrame(pdf.astype(str)).filter('Project_ID is not NULL')\n",
    "df = df.filter('Project_ID is not NULL').union(ddd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = df.select('Loan_Number','Country','Country_Code').distinct().toPandas().set_index('Loan_Number').T.to_dict('list')\n",
    "c1b = df.filter('Guarantor == \\'UNITED KINGDOM\\'').select('Loan_Number').distinct().collect()\n",
    "for i in c1b:\n",
    "    c1[i.Loan_Number] = ['UNITED KINGDOM','GB']\n",
    "pdf = df.filter('Guarantor is NULL or Guarantor_Country_Code is NULL').toPandas()\n",
    "for index,row in pdf.iterrows():\n",
    "    att = row.Loan_Number\n",
    "    if att in c1.keys():\n",
    "        row.Guarantor = c1.get(att)[0]\n",
    "        row.Guarantor_Country_Code = c1.get(att)[1]\n",
    "ddd = spark.createDataFrame(pdf).filter('Guarantor is not NULL and Guarantor_Country_Code is not NULL')\n",
    "df = df.filter('Guarantor is not NULL and Guarantor_Country_Code is not NULL').union(ddd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = df.filter('Interest_Rate is not NULL').filter('Interest_Rate != \\'None\\'').select('Loan_Number','Interest_Rate').distinct().toPandas()\n",
    "i1.Interest_Rate = i1.Interest_Rate.astype(np.float16)\n",
    "i1 = i1.groupby('Loan_Number', as_index=True).agg({\"Interest_Rate\": \"mean\"})['Interest_Rate'].to_dict()\n",
    "pdf = df.filter('Interest_Rate is NULL').toPandas()\n",
    "for index,row in pdf.iterrows():\n",
    "    att = row.Interest_Rate\n",
    "    if att in c1.keys():\n",
    "        row.Interest_Rate = c1.get(att)\n",
    "ddd = spark.createDataFrame(pdf.astype(str)).filter('Interest_Rate is not NULL')\n",
    "df = df.filter('Interest_Rate is not NULL').union(ddd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(df, column):\n",
    "    x1 = df.filter('{} is not NULL'.format(column)).filter('{} != \\'None\\''.format(column)).select('Loan_Number','{}'.format(column)).distinct().toPandas().set_index('Loan_Number')['{}'.format(column)].to_dict()\n",
    "    pdf = df.filter('{} is NULL'.format(column)).toPandas()\n",
    "    for index,row in pdf.iterrows():\n",
    "        att = row.Loan_Number\n",
    "        if att in x1.keys():\n",
    "            row.column = x1.get(att)\n",
    "    ddd = spark.createDataFrame(pdf.astype(str)).filter('{} is not NULL'.format(column)).filter('{} != \\'None\\''.format(column))\n",
    "    df = df.filter('{} is not NULL'.format(column)).union(ddd)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>End_of_Period</th>\n",
       "      <th>Loan_Number</th>\n",
       "      <th>Region</th>\n",
       "      <th>Country_Code</th>\n",
       "      <th>Country</th>\n",
       "      <th>Guarantor_Country_Code</th>\n",
       "      <th>Guarantor</th>\n",
       "      <th>Loan_Type</th>\n",
       "      <th>Loan_Status</th>\n",
       "      <th>Interest_Rate</th>\n",
       "      <th>Project_ID</th>\n",
       "      <th>Original_Principal_Amount</th>\n",
       "      <th>Cancelled_Amount</th>\n",
       "      <th>Undisbursed_Amount</th>\n",
       "      <th>Disbursed_Amount</th>\n",
       "      <th>Repaid_to_IBRD</th>\n",
       "      <th>Due_to_IBRD</th>\n",
       "      <th>Exchange_Adjustment</th>\n",
       "      <th>Borrowers_Obligation</th>\n",
       "      <th>Sold_3rd_Party</th>\n",
       "      <th>Repaid_3rd_Party</th>\n",
       "      <th>Due_3rd_Party</th>\n",
       "      <th>Loans_Held</th>\n",
       "      <th>First_Repayment_Date</th>\n",
       "      <th>Last_Repayment_Date</th>\n",
       "      <th>Agreement_Signing_Date</th>\n",
       "      <th>Board_Approval_Date</th>\n",
       "      <th>Effective_Date_Most_Recent</th>\n",
       "      <th>Closed_Date_Most_Recent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   End_of_Period  Loan_Number  Region  Country_Code  Country  \\\n",
       "0              0            0       0             0        0   \n",
       "\n",
       "   Guarantor_Country_Code  Guarantor  Loan_Type  Loan_Status  Interest_Rate  \\\n",
       "0                       0          0          0            0              0   \n",
       "\n",
       "   Project_ID  Original_Principal_Amount  Cancelled_Amount  \\\n",
       "0           0                          0                 0   \n",
       "\n",
       "   Undisbursed_Amount  Disbursed_Amount  Repaid_to_IBRD  Due_to_IBRD  \\\n",
       "0                   0                 0               0            0   \n",
       "\n",
       "   Exchange_Adjustment  Borrowers_Obligation  Sold_3rd_Party  \\\n",
       "0                    0                     0               0   \n",
       "\n",
       "   Repaid_3rd_Party  Due_3rd_Party  Loans_Held  First_Repayment_Date  \\\n",
       "0                 0              0           0                     0   \n",
       "\n",
       "   Last_Repayment_Date  Agreement_Signing_Date  Board_Approval_Date  \\\n",
       "0                    0                       0                    0   \n",
       "\n",
       "   Effective_Date_Most_Recent  Closed_Date_Most_Recent  \n",
       "0                           0                        0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = clean(df,'First_Repayment_Date')\n",
    "df = clean(df,'Agreement_Signing_Date')\n",
    "df = clean(df,'Closed_Date_Most_Recent')\n",
    "df = clean(df,'Effective_Date_Most_Recent')\n",
    "df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate country table \n",
    "df_country = df.select('Country_Code','Country','Region').distinct()\n",
    "newRow = spark.createDataFrame([('GB','United Kingdom','EUROPE AND CENTRAL ASIA')])\n",
    "df_country = df_country.union(newRow)\n",
    "df_country = df_country.withColumn('Country_Id',row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "#df_country.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate time table \n",
    "df_time = df.select('Loan_Number','First_Repayment_Date','Last_Repayment_Date','Agreement_Signing_Date','Board_Approval_Date','Effective_Date_Most_Recent','Closed_Date_Most_Recent').distinct()\n",
    "df_time = df_time.withColumn('Time_Id',row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "#df_time.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Amount table \n",
    "df_amount = df.select('Loan_Number','Original_Principal_Amount','Cancelled_Amount','Undisbursed_Amount','Disbursed_Amount','Repaid_to_IBRD','Due_to_IBRD','Sold_3rd_Party','Repaid_3rd_Party', 'Due_3rd_Party', 'Loans_Held').distinct()\n",
    "df_amount = df_amount.withColumn('Amount_Id',row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "#df_amount.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Loan_Type table \n",
    "df_loan_type = df.select('Loan_Type').distinct()\n",
    "df_loan_type = df_loan_type.withColumn('Loan_Type_Id',row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "#df_loan_type.limit(2).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1355.7922852039337 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Generate Loan_Status table \n",
    "df_loan_status = df.select('Loan_Status').distinct()\n",
    "df_loan_status = df_loan_status.withColumn('Loan_Status_Id',row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "#df_loan_status.limit(2).toPandas()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country.createOrReplaceTempView(\"country\")\n",
    "df_time.createOrReplaceTempView(\"time\")\n",
    "df_amount.createOrReplaceTempView(\"amount\")\n",
    "df_loan_type.createOrReplaceTempView(\"loan_type\")\n",
    "df_loan_status.createOrReplaceTempView(\"loan_status\")\n",
    "df.createOrReplaceTempView(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_norm = spark.sql(\"\"\"\n",
    "    select l.Loan_Number, l.End_of_Period, l.Interest_Rate, l.Project_ID, l.Exchange_Adjustment, l.Borrowers_Obligation,  \\\n",
    "    c.Country_Id, cc.Country_Id as Guarantor_Country_Id, t.Time_Id, a.Amount_Id, lt.Loan_Type_Id, ls.Loan_Status_Id \\\n",
    "    from ((country c \\\n",
    "    join log l on c.Country_Code = l.Country_Code and c.Country = l.Country and c.Region = l.Region) \\\n",
    "    join country cc on cc.Country = l.Guarantor) \\\n",
    "    join time t on t.Loan_Number = l.Loan_Number \\\n",
    "    join amount a on a.Loan_Number = l.Loan_Number \\\n",
    "    join loan_type lt on lt.Loan_Type = l.Loan_Type \\\n",
    "    join loan_status ls on ls.Loan_Status = l.Loan_Status\n",
    "    \"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_norm.limit(1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_norm.createOrReplaceTempView(\"log_norm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_norm1 = spark.sql(\"\"\"\n",
    "    select Loan_Number, Country_Id,  from log_norm c join log l on c.Country_Code = l.Country_Code and c.Country = l.Country and c.Region = l.Region\n",
    "    \"\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "print ([item for item, count in collections.Counter(t).items() if count > 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\"\n",
    "    This function is used to load songs data from s3 to our data lake and export as parquet file back to my s3 folder.\n",
    "    \"\"\"\n",
    "    # get filepath to song data file\n",
    "    song_data = input_data + 'song_data/A/A/A/*'\n",
    "    # uncomment if you want to load all data\n",
    "    #song_data = input_data + 'song_data/*/*/*/*'\n",
    "    \n",
    "    # read song data file, using song_data/A/A/A/* for performance\n",
    "    df = spark.read.json(song_data)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df.select('song_id', 'title', 'artist_id', 'year', 'duration')\n",
    "    \n",
    "    # convert the data type to proper data type for each column\n",
    "    fields_1 = {'song_id':'string','title':'string', 'artist_id':'string', 'year':'int', 'duration':'float'}\n",
    "    exprs_1 = [ \"cast ({} as {})\".format(key,value) for key, value in fields_1.items()]\n",
    "    songs_table = songs_table.selectExpr(*exprs_1)\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.partitionBy('year','artist_id').parquet(output_data + \"songs.parquet\")\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df.select('artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude')\n",
    "    \n",
    "    # convert the data type to proper data type for each column\n",
    "    fields_2 = {'artist_id':'string', 'artist_name':'string', 'artist_location':'string', 'artist_latitude':'string', 'artist_longitude':'string'}\n",
    "    exprs_2 = [ \"cast ({} as {})\".format(key,value) for key, value in fields_2.items()]\n",
    "    songs_table = artists_table.selectExpr(*exprs_2)\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.parquet(output_data + \"artists.parquet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-east-1\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )\n",
    "## This is to display what we have in S3 \n",
    "sampleDbBucket =  s3.Bucket(\"john-udacity-s3\")\n",
    "i = 0\n",
    "for obj in sampleDbBucket.objects.filter(Prefix=\"movie\"):\n",
    "    print(obj)\n",
    "    i += 1\n",
    "    if i > 10:\n",
    "        break  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
