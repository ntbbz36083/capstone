{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Data Pipeline for International Bank for Reconstruction and Development(IBRD)\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The International Bank for Reconstruction and Development, wants to move their processes and data pipeline onto the cloud. Their data resides in S3, in a directory of CSV on the transcation history of the loans. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Clean and Save the Data\n",
    "* Step 4: Define the Data Model\n",
    "* Step 5: Run ETL to Model the Data\n",
    "* Step 6: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "* Project Description\n",
    "\n",
    "In this project, we will build an ETL pipeline that extracts data from S3, processes and clean them using Spark, and loads the data back into S3 as a set of dimensional tables in csv format. Then, create cluster with redshift and load data into redshift. This will allow our analytics team to continue finding insights of our loan users.\n",
    "\n",
    "* Tool\n",
    "\n",
    " In this porject, we will use AWS S3, Redshift, Spark, Python\n",
    " \n",
    "#### Describe and Gather Data \n",
    "The data is coming from World Bank and you can download it from below page:\n",
    "\n",
    "https://finances.worldbank.org/Loans-and-Credits/IBRD-Statement-Of-Loans-Historical-Data/zucq-nrc3\n",
    "\n",
    "It is around 300mb and for data layout of this file, please see the data layout file in the workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all modules and setup here\n",
    "# Loading all library\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import StringType\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "import boto3\n",
    "import time\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "# Change padans parameter to adjust visliazation\n",
    "pd.set_option('max_colwidth', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "# Load in aws credential\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "KEY = config.get('AWS','KEY')\n",
    "SECRET = config.get('AWS','SECRET')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['KEY']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that creates spark session\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    This function is used to create a spark session to work in\n",
    "    \"\"\"\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "# Create spark session\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from AWS s3\n",
    "df_spark = spark.read.csv(\"s3a://udacity-leejohn/loan/ibrd-statement-of-loans-historical-data.csv\", header=True)\n",
    "# Function that will uppercase everything in the dataframe\n",
    "fields = df_spark.schema.fields\n",
    "stringFields = filter(lambda f: isinstance(f.dataType, StringType), fields)\n",
    "nonStringFields = map(lambda f: col(f.name), filter(lambda f: not isinstance(f.dataType, StringType), fields))\n",
    "stringFieldsTransformed = map(lambda f: upper(col(f.name)), stringFields) \n",
    "allFields = [*stringFieldsTransformed, *nonStringFields]\n",
    "df_new = df_spark.select(allFields)\n",
    "# Rename the column name\n",
    "# Get old column names \n",
    "oldColumns = df_new.schema.names\n",
    "# Setup new column names\n",
    "newColumns  = ['End_of_Period', 'Loan_Number', 'Region', 'Country_Code', 'Country','Borrower','Guarantor_Country_Code','Guarantor','Loan_Type','Loan_Status','Interest_Rate','Currency_of_Commitment','Project_ID','Project_Name','Original_Principal_Amount','Cancelled_Amount','Undisbursed_Amount','Disbursed_Amount','Repaid_to_IBRD','Due_to_IBRD','Exchange_Adjustment','Borrowers_Obligation','Sold_3rd_Party','Repaid_3rd_Party','Due_3rd_Party','Loans_Held','First_Repayment_Date','Last_Repayment_Date','Agreement_Signing_Date','Board_Approval_Date','Effective_Date_Most_Recent','Closed_Date_Most_Recent','Last_Disbursement_Date']\n",
    "# Rename the dataframe\n",
    "df = reduce(lambda df_spark, idx: df_spark.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of rows in data\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the data\n",
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to display the number of null value of each column, before we do any cleaning, you will see null data count for each column with below command\n",
    "df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Cleaning Steps\n",
    "\n",
    "In this step, we will clean data to remove any null value with different methods. \n",
    "This process will take about 8 min."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since below 2 columns have too much null values(average 50% null values) and are not necessary, so we drop them.  \n",
    "df = df.drop('Last_Disbursement_Date')\n",
    "df = df.drop('Currency_of_Commitment')\n",
    "# For these 2 columns, if we look into the raw data,we will see that there are some \"?\", \"@\" in the data, which are some errors come from raw data. \n",
    "# I think this is the error comes from different coding type.\n",
    "# Instead of trying to fix them, we can just drop them.\n",
    "df = df.drop('Borrower')\n",
    "df = df.drop('Project_Name')\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the loan_number to make it 9 digits\n",
    "# From data layout, we can know that loan_number should follow a 9 dights pattern.\n",
    "# Find records that have less than 9 digits Loan_Number, replace them to the correct format\n",
    "df_6 = df.where(length(col(\"Loan_Number\")) == 6).withColumn(\"Loan_Number\", regexp_replace(col(\"Loan_Number\") ,  \"(\\\\w{4})(\\\\d{2})\" , \"$1000$2\" ))\n",
    "df_7 = df.where(length(col(\"Loan_Number\")) == 7).withColumn(\"Loan_Number\", regexp_replace(col(\"Loan_Number\") ,  \"(\\\\w{4})(\\\\d{3})\" , \"$100$2\" ))\n",
    "df_8 = df.where(length(col(\"Loan_Number\")) == 8).withColumn(\"Loan_Number\", regexp_replace(col(\"Loan_Number\") ,  \"(\\\\w{4})(\\\\d{4})\" , \"$10$2\" ))\n",
    "\n",
    "# No records with loan_number that has less than 9 digits\n",
    "df = df.filter(length(col(\"Loan_Number\")) == 9).union(df_6).union(df_7).union(df_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If both Guarantor_Country_Code and Guarantor are empty, then it's hard to say whether they are suppose to be empty(no guarantor)\n",
    "# or they are missing values, so I just drop them.\n",
    "df = df.filter('Guarantor_Country_Code is not NULL or Guarantor is not NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each loan number, there should be one country code. Run below code, we will find there are 3 records that have the wrong country data, so we drop them. \n",
    "#df.select('Loan_Number','Country_Code').distinct().groupBy('Loan_Number').count().withColumnRenamed('count', 'ccount').filter('count>1').toPandas()\n",
    "#df.where(df.Loan_Number == 'IBRD82610').select('Country').groupBy('Country').count().toPandas()\n",
    "#df.where(df.Loan_Number == 'IBRD82550').select('Country').groupBy('Country').count().toPandas()\n",
    "#df.where(df.Loan_Number == 'IBRD82580').select('Country').groupBy('Country').count().toPandas()\n",
    "df = df.where((df.Loan_Number == 'IBRD82580') & (df.Country != 'CHINA') | (df.Loan_Number != 'IBRD82580'))\n",
    "df = df.where((df.Loan_Number == 'IBRD82550') & (df.Country != 'CHINA') | (df.Loan_Number != 'IBRD82550'))\n",
    "df = df.where((df.Loan_Number == 'IBRD82610') & (df.Country != 'INDIA') | (df.Loan_Number != 'IBRD82610'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all missing value for Project_ID column\n",
    "# Create a dict that key is loan_number and value is project_id\n",
    "x1 = df.filter('Project_ID is not NULL').select('Loan_Number','Project_ID').distinct().toPandas().set_index('Loan_Number')['Project_ID'].to_dict() \n",
    "# Create a pandas dataframe that only has the records that have missing project_id\n",
    "pdf = df.filter('Project_ID is NULL').toPandas()\n",
    "# Loop through the dataframe and replace its missing value with the loan_number\n",
    "for index,row in pdf.iterrows():\n",
    "    att = row.Loan_Number\n",
    "    if att in x1.keys():\n",
    "        row.Project_ID = x1.get(att)\n",
    "# Convert it back to a spark dataframe\n",
    "ddd = spark.createDataFrame(pdf.astype(str)).filter('Project_ID is not NULL').filter('Project_ID != \\'None\\'')\n",
    "# Union it with the good records and get new dataframe\n",
    "df = df.filter('Project_ID is not NULL').union(ddd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all missing value for Guarantor and Guarantor_Country_Code column\n",
    "# Create a dict that key is loan_number and value is a array of country, country_code\n",
    "c1 = df.select('Loan_Number','Country','Country_Code').distinct().toPandas().set_index('Loan_Number').T.to_dict('list')\n",
    "# By exploring data, we find there is one more country in Guarantor, which is united kingdom, we will add it to our dict\n",
    "c1b = df.filter('Guarantor == \\'UNITED KINGDOM\\'').select('Loan_Number').distinct().collect()\n",
    "for i in c1b:\n",
    "    c1[i.Loan_Number] = ['UNITED KINGDOM','GB']\n",
    "# Loop through the dataframe and replace its missing value with the loan_number\n",
    "pdf = df.filter('Guarantor is NULL or Guarantor_Country_Code is NULL').toPandas()\n",
    "for index,row in pdf.iterrows():\n",
    "    att = row.Loan_Number\n",
    "    if att in c1.keys():\n",
    "        row.Guarantor = c1.get(att)[0]\n",
    "        row.Guarantor_Country_Code = c1.get(att)[1]\n",
    "# Convert it back to a spark dataframe\n",
    "ddd = spark.createDataFrame(pdf).filter('Guarantor is not NULL and Guarantor_Country_Code is not NULL')\n",
    "# Union it with the good records and get new dataframe\n",
    "df = df.filter('Guarantor is not NULL and Guarantor_Country_Code is not NULL').union(ddd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all missing value for Interest_Rate\n",
    "# Since for each loan number, we will find different interest rate. So to handle this case, for rach loan number, I will use mean value of all intertest rate to repalce its nul value. \n",
    "# Create a dict that key is loan_number and value is intertest rate\n",
    "i1 = df.filter('Interest_Rate is not NULL').filter('Interest_Rate != \\'None\\'').select('Loan_Number','Interest_Rate').distinct().toPandas()\n",
    "i1.Interest_Rate = i1.Interest_Rate.astype(np.float16)\n",
    "i1 = i1.groupby('Loan_Number', as_index=True).agg({\"Interest_Rate\": \"mean\"})['Interest_Rate'].to_dict()\n",
    "pdf = df.filter('Interest_Rate is NULL').toPandas()\n",
    "# Loop through the dataframe and replace its missing value with the loan_number\n",
    "for index,row in pdf.iterrows():\n",
    "    att = row.Interest_Rate\n",
    "    if att in c1.keys():\n",
    "        row.Interest_Rate = c1.get(att)\n",
    "# Convert it back to a spark dataframe\n",
    "ddd = spark.createDataFrame(pdf.astype(str)).filter('Interest_Rate is not NULL').filter('Interest_Rate != \\'None\\'')\n",
    "# Union it with the good records and get new dataframe\n",
    "df = df.filter('Interest_Rate is not NULL').union(ddd)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to display the number of null value of each column, before we do any cleaning, you will see null data count for each column with below command\n",
    "df.select(*(sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns)).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After above steps, we wouldn't see any missing value except for the date related columns.\n",
    "For date related columns, since they have more problems, we are not necessarily to remove all missing value. We have another way to deal with them.\n",
    "\n",
    "There are 2 issues with date related columns:\n",
    "1. Missing values\n",
    "2. Having more than one date.\n",
    "For each loan_number, we should only have one unique set of [First_Repayment_Date,Last_Repayment_Date,Agreement_Signing_Date,Board_Approval_Date,Effective_Date_Most_Recent,Closed_Date_Most_Recent],\n",
    "However, we will see there are more than one set.\n",
    "\n",
    "To handle this case, for each date related column, we can find the most frequent date of each loan number and use it as its only date. By doing this, we can find a unique set of date values for each loan number.\n",
    "Therefore, we can create time table directly. Later we will need to create the transaction data, we can ignore the raw data of these date related columns and use loan number to recreate these columns.\n",
    "This will also help us prevent spending time on cleaning missing data, so we can jump into data modeling directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define the Data Model\n",
    "#### Conceptual Data Model\n",
    "In this project, I am using fact-dimensional data modeling method. The reason I am using this method is that it will be much more straightforward and easy for users.\n",
    "We will have 1 fact table and 5 dimensional tables.\n",
    "\n",
    "##### Fact table:\n",
    "trainsaction:Loan_Number, Time_Id, Country_Id, Guarantor_Country_Id, Loan_Type, Loan_Status_Id, Amount_Id, End_of_Period, Interest_Rate, Project_ID, Exchange_Adjustment, Borrowers_Obligation, \n",
    "Cancelled_Amount,Undisbursed_Amount, Disbursed_Amount, Repaid_to_IBRD, Due_to_IBRD, Loans_Held\n",
    "\n",
    "##### Dimension table\n",
    "\n",
    "df_time: Time_Id, First_Repayment_Date, Last_Repayment_Date, Board_Approval_Date, Agreement_Signing_Date, Effective_Date_Most_Recent, Closed_Date_Most_Recent\n",
    "\n",
    "df_country: Country_Id, Country_Code, Country, Region\n",
    "\n",
    "df_amount: Amount_Id, Original_Principal_Amount, Sold_3rd_Party, Repaid_3rd_Party, Due_3rd_Party\n",
    "\n",
    "df_loan_type: Loan_Type_Id, Loan_Type\n",
    "\n",
    "df_loan_status: Loan_Status_Id, Loan_Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate time dataframe \n",
    "# By exploring data, we can see that for each loan number, it should only have one set of dates related column. However, we will find for date related columns, it has more than one combination. so I crete this function to find its more frequent value for each loan number. \n",
    "def getss(df, column):\n",
    "    # Create a sql table with loan number and column\n",
    "    df.select('Loan_Number', column).filter('{} is not NULL'.format(column)).filter('{} != \\'None\\''.format(column)).createOrReplaceTempView(\"time\")\n",
    "    # Run query to get the most frequent value of each category for each loan number\n",
    "    x = spark.sql(\"\"\"\n",
    "    SELECT Loan_Number, {} FROM \n",
    "    (\n",
    "    SELECT Loan_Number, {}, count(1) total_records, ROW_NUMBER() OVER (PARTITION BY Loan_Number ORDER BY count(1) desc) AS seqnum\n",
    "    FROM time \n",
    "    group by Loan_Number, {}\n",
    "    )\n",
    "    WHERE seqnum = 1\n",
    "    \"\"\".format(column, column, column))\n",
    "    return x\n",
    "# Getting the most frequent value of each column for each loan number\n",
    "x1 = getss(df, 'First_Repayment_Date')\n",
    "x2 = getss(df, 'Last_Repayment_Date')\n",
    "x3 = getss(df, 'Agreement_Signing_Date')\n",
    "x4 = getss(df, 'Board_Approval_Date')\n",
    "x5 = getss(df, 'Effective_Date_Most_Recent')\n",
    "x6 = getss(df, 'Closed_Date_Most_Recent')\n",
    "# Combine them and drop the duplicate key\n",
    "df_time = x1.join(x2, x1.Loan_Number == x2.Loan_Number).drop(x2.Loan_Number)\n",
    "df_time = df_time.join(x3, df_time.Loan_Number == x3.Loan_Number).drop(x3.Loan_Number)\n",
    "df_time = df_time.join(x4, df_time.Loan_Number == x4.Loan_Number).drop(x4.Loan_Number)\n",
    "df_time = df_time.join(x5, df_time.Loan_Number == x5.Loan_Number).drop(x5.Loan_Number)\n",
    "df_time = df_time.join(x6, df_time.Loan_Number == x6.Loan_Number).drop(x6.Loan_Number)\n",
    "# Add a unique id Time_Id for this dataframe\n",
    "df_time = df_time.withColumn('Time_Id',row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "# Rename its column name\n",
    "df_time = df_time.selectExpr('Time_Id', 'Loan_Number', 'First_Repayment_Date as First_Repayment_Date_t', 'Last_Repayment_Date as Last_Repayment_Date_t', 'Agreement_Signing_Date as Agreement_Signing_Date_t','Board_Approval_Date \\\n",
    "                         as Board_Approval_Date_t','Effective_Date_Most_Recent as Effective_Date_Most_Recent_t','Closed_Date_Most_Recent as Closed_Date_Most_Recent_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate country dataframe \n",
    "df_country = df.select('Country_Code','Country','Region').distinct()\n",
    "newRow = spark.createDataFrame([('GB','United Kingdom','EUROPE AND CENTRAL ASIA')])\n",
    "df_country = df_country.union(newRow)\n",
    "# Add a unique id Country_Id for this dataframe\n",
    "df_country = df_country.withColumn('Country_Id',row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "# Casting to the right data type\n",
    "fields = {'Country_Code':'string', 'Country':'string', 'Region':'string', 'Country_Id':'integer'}\n",
    "exprs = [ \"cast ({} as {})\".format(key,value) for key, value in fields.items()]\n",
    "df_country = df_country.selectExpr(*exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Amount dataframe \n",
    "df_amount = df.select('Loan_Number','Original_Principal_Amount','Sold_3rd_Party','Repaid_3rd_Party', 'Due_3rd_Party').distinct()\n",
    "# Add a unique id Amount_Id for this dataframe\n",
    "df_amount = df_amount.withColumn('Amount_Id',row_number().over(Window.orderBy(monotonically_increasing_id())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Loan_Type dataframe \n",
    "df_loan_type = df.select('Loan_Type').distinct()\n",
    "# Add a unique id Loan_Type_Id for this dataframe\n",
    "df_loan_type = df_loan_type.withColumn('Loan_Type_Id',row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "# Casting to the right data type\n",
    "fields = {'Loan_Type':'string', 'Loan_Type_Id':'integer'}\n",
    "exprs = [ \"cast ({} as {})\".format(key,value) for key, value in fields.items()]\n",
    "df_loan_type = df_loan_type.selectExpr(*exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Loan_Status dataframe \n",
    "df_loan_status = df.select('Loan_Status').distinct()\n",
    "# Add a unique id Loan_Status_Id for this dataframe\n",
    "df_loan_status = df_loan_status.withColumn('Loan_Status_Id',row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "# Casting to the right data type\n",
    "fields = {'Loan_Status':'string', 'Loan_Status_Id':'integer'}\n",
    "exprs = [ \"cast ({} as {})\".format(key,value) for key, value in fields.items()]\n",
    "df_loan_status = df_loan_status.selectExpr(*exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sql tables \n",
    "df_country.createOrReplaceTempView(\"country\")\n",
    "df_time.createOrReplaceTempView(\"time\")\n",
    "df_amount.createOrReplaceTempView(\"amount\")\n",
    "df_loan_type.createOrReplaceTempView(\"loan_type\")\n",
    "df_loan_status.createOrReplaceTempView(\"loan_status\")\n",
    "df.createOrReplaceTempView(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate transaction dataframe by joing log table with the other tables on loan number\n",
    "trainsaction = spark.sql(\"\"\"\n",
    "    select l.Loan_Number, t.Time_Id, c.Country_Id, cc.Country_Id as Guarantor_Country_Id, lt.Loan_Type, ls.Loan_Status_Id, a.Amount_Id,\n",
    "    l.End_of_Period, l.Interest_Rate, l.Project_ID, l.Exchange_Adjustment, l.Borrowers_Obligation, l.Cancelled_Amount,\n",
    "    l.Undisbursed_Amount, l.Disbursed_Amount, l.Repaid_to_IBRD, l.Due_to_IBRD, l.Loans_Held\n",
    "    from log l \n",
    "    join country c on l.Country_Code= c.Country_Code\n",
    "    join country cc on l.Guarantor = cc.Country \n",
    "    join loan_type lt on l.Loan_Type = lt.Loan_Type \n",
    "    join loan_status ls on l.Loan_Status = ls.Loan_Status\n",
    "    join amount a on l.Loan_Number = a.Loan_Number\n",
    "    join time t on l.Loan_Number = t.Loan_Number\n",
    "    \"\"\")\n",
    "# Casting to the right data type\n",
    "fields = {'Loan_Number':'string', 'Time_Id':'integer', 'Country_Id':'integer', 'Guarantor_Country_Id':'integer',\\\n",
    "          'Loan_Type':'string','Loan_Status_Id':'integer', 'Amount_Id':'integer', 'End_of_Period':'timestamp', \\\n",
    "          'Interest_Rate':'float', 'Project_ID':'string', 'Exchange_Adjustment':'float','Borrowers_Obligation':'float',\\\n",
    "          'Cancelled_Amount':'float', 'Undisbursed_Amount':'float', 'Disbursed_Amount':'float','Repaid_to_IBRD':'float', 'Due_to_IBRD':'float', 'Loans_Held':'float'}\n",
    "exprs = [ \"cast ({} as {})\".format(key,value) for key, value in fields.items()]\n",
    "trainsaction = trainsaction.selectExpr(*exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the loan number column from the dataframes\n",
    "df_amount_final = df_amount.select('Amount_Id','Original_Principal_Amount','Sold_3rd_Party','Repaid_3rd_Party', 'Due_3rd_Party')\n",
    "# Casting to the right data type\n",
    "fields = {'Amount_Id':'integer', 'Original_Principal_Amount':'float', 'Sold_3rd_Party':'float', 'Repaid_3rd_Party':'float', 'Due_3rd_Party':'float'}\n",
    "exprs = [ \"cast ({} as {})\".format(key,value) for key, value in fields.items()]\n",
    "df_amount_final = df_amount_final.selectExpr(*exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column names\n",
    "df_time_final = df_time.selectExpr('Time_Id', 'First_Repayment_Date_t as First_Repayment_Date', \\\n",
    "                                   'Last_Repayment_Date_t as Last_Repayment_Date','Agreement_Signing_Date_t as Agreement_Signing_Date',\\\n",
    "                                   'Board_Approval_Date_t as Board_Approval_Date','Effective_Date_Most_Recent_t as Effective_Date_Most_Recent',\\\n",
    "                                   'Closed_Date_Most_Recent_t as Closed_Date_Most_Recent')\n",
    "# Casting to the right data type\n",
    "fields = {'Time_Id':'integer', 'First_Repayment_Date':'timestamp', 'Last_Repayment_Date':'timestamp', \\\n",
    "          'Agreement_Signing_Date':'timestamp', 'Board_Approval_Date':'timestamp', 'Effective_Date_Most_Recent':'timestamp', 'Closed_Date_Most_Recent':'timestamp'}\n",
    "exprs = [ \"cast ({} as {})\".format(key,value) for key, value in fields.items()]\n",
    "df_time_final = df_time_final.selectExpr(*exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataframe that contains the counts of each table\n",
    "a = df_country.count()\n",
    "b = df_time_final.count()\n",
    "c = df_amount_final.count()\n",
    "d = df_loan_type.count()\n",
    "e = df_loan_status.count()\n",
    "f = trainsaction.count()\n",
    "data = {'Table_name':['df_country', 'df_time_final', 'df_amount_final', 'df_loan_type','df_loan_status','trainsaction'], \n",
    "        'Count':[a, b, c, d, e, f]}\n",
    "df_count = pd.DataFrame(data)\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving files to S3 \n",
    "This process will take about 20 minutes. I already saved these files on AWS and you can ignore this part and go to next part directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if you want to save data to AWS. The data is already available on aws in this path: udacity-leejohn-w2/loan, will setup path here as loan_test to prevent duplicate error\n",
    "#start_time = time.time()\n",
    "#df_country.write.csv(\"s3a://udacity-leejohn-w2/loan_test/country\")\n",
    "#df_time_final.write.csv(\"s3a://udacity-leejohn-w2/loan_test/time\")\n",
    "#df_loan_status.write.csv(\"s3a://udacity-leejohn-w2/loan_test/loan_status\")\n",
    "#df_loan_type.write.csv(\"s3a://udacity-leejohn-w2/loan_test/loan_type\")\n",
    "#df_amount_final.write.csv(\"s3a://udacity-leejohn-w2/loan_test/amount\")\n",
    "#trainsaction.write.csv(\"s3a://udacity-leejohn-w2/loan_test/trainsaction\")\n",
    "#print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this command to see the file on AWS\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )\n",
    "## This is to display what we have in S3 \n",
    "sampleDbBucket =  s3.Bucket(\"udacity-leejohn-w2\")\n",
    "i = 0\n",
    "for obj in sampleDbBucket.objects.filter(Prefix=\"loan\"):\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Run ETL to load the Data\n",
    "First, run below script will create a redshift cluster and it will take about 5-10 min. \n",
    "\n",
    "Second, when you see \"cluster is available\", you can go ahread and run the etl.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Param       Value\n",
      "0        DWH_CLUSTER_TYPE  multi-node\n",
      "1           DWH_NUM_NODES           4\n",
      "2           DWH_NODE_TYPE   dc2.large\n",
      "3  DWH_CLUSTER_IDENTIFIER    redshift\n",
      "4                  DWH_DB         dwh\n",
      "5             DWH_DB_USER     dwhuser\n",
      "6         DWH_DB_PASSWORD    Passw0rd\n",
      "7                DWH_PORT        5439\n",
      "8       DWH_IAM_ROLE_NAME     dwhRole\n",
      "1.1 Creating a new IAM Role\n",
      "1.2 Attaching Policy\n",
      "1.3 Get the IAM role ARN\n",
      "arn:aws:iam::638983295418:role/dwhRole\n",
      "1.4 Creating Cluster\n",
      "We are still working on creating the cluster, approximate 0/20 done!\n",
      "We are still working on creating the cluster, approximate 1/20 done!\n",
      "We are still working on creating the cluster, approximate 2/20 done!\n",
      "We are still working on creating the cluster, approximate 3/20 done!\n",
      "We are still working on creating the cluster, approximate 4/20 done!\n",
      "We are still working on creating the cluster, approximate 5/20 done!\n",
      "We are still working on creating the cluster, approximate 6/20 done!\n",
      "We are still working on creating the cluster, approximate 7/20 done!\n",
      "We are still working on creating the cluster, approximate 8/20 done!\n",
      "We are still working on creating the cluster, approximate 9/20 done!\n",
      "We are still working on creating the cluster, approximate 10/20 done!\n",
      "cluster is already available!\n",
      "DWH_ENDPOINT ::  redshift.cqvdryicbxdk.us-west-2.redshift.amazonaws.com\n",
      "DWH_ROLE_ARN ::  arn:aws:iam::638983295418:role/dwhRole\n",
      "ec2.SecurityGroup(id='sg-0631784a')\n",
      "Policy created, you can go ahead!\n"
     ]
    }
   ],
   "source": [
    "# This script will create a redshift cluster automatically, please don't move forward until you see \"cluster is available!\" and \"Policy created, you can go ahead!\"\n",
    "%run -i 'Redshift_Create.py'\n",
    "# This will take 5-10 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All table dropped!\n",
      "All table created!\n",
      "All table loaded!\n"
     ]
    }
   ],
   "source": [
    "# This script will drop, create and load table into redshift cluster. You will see \"All table dropped! All table created! All table loaded!\" if everything works as exptected.\n",
    "%run -i 'etl.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Quality Checks\n",
    "From here, we will first check Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    "\n",
    "Then, we will count on each table to make sure there's no empty table. This ensures that the scripts are doing the right things.\n",
    "\n",
    "Last, we will compare the counts between target table and spark dataframe to ensure data is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql://dwhuser:Passw0rd@redshift.cqvdryicbxdk.us-west-2.redshift.amazonaws.com:5439/dwh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Connected: dwhuser@dwh'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "%load_ext sql\n",
    "DWH_DB_USER = config.get('CLUSTER','DB_USER')\n",
    "DWH_DB_PASSWORD = config.get('CLUSTER','DB_PASSWORD')\n",
    "DWH_ENDPOINT = config.get('CLUSTER','HOST')\n",
    "DWH_PORT = config.get('CLUSTER','DB_PORT')\n",
    "DWH_DB = config.get('CLUSTER','DB_NAME')\n",
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the count of each table from spark dataframe and we can compare this with the count of target table on redshift to make sure everything is loaded correctly.\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * postgresql://dwhuser:***@redshift.cqvdryicbxdk.us-west-2.redshift.amazonaws.com:5439/dwh\n",
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>8152</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(8152,)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql select count(*) from amount;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select * from PG_TABLE_DEF where schemaname = 'public' and tablename = 'amount'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select count(*) from country;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select * from PG_TABLE_DEF where schemaname = 'public' and tablename = 'country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select count(*) from loan_status;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select * from PG_TABLE_DEF where schemaname = 'public' and tablename = 'loan_status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select count(*) from loan_type;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select * from PG_TABLE_DEF where schemaname = 'public' and tablename = 'loan_type'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select count(*) from time;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select * from PG_TABLE_DEF where schemaname = 'public' and tablename = 'time'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select count(*) from transaction;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql select * from PG_TABLE_DEF where schemaname = 'public' and tablename = 'transaction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Param       Value\n",
      "0  DWH_CLUSTER_TYPE        multi-node\n",
      "1  DWH_NUM_NODES           4         \n",
      "2  DWH_NODE_TYPE           dc2.large \n",
      "3  DWH_CLUSTER_IDENTIFIER  redshift  \n",
      "4  DWH_DB                  dwh       \n",
      "5  DWH_DB_USER             dwhuser   \n",
      "6  DWH_DB_PASSWORD         Passw0rd  \n",
      "7  DWH_PORT                5439      \n",
      "8  DWH_IAM_ROLE_NAME       dwhRole   \n",
      "We are still working on deleting the cluster!\n",
      "We are still working on deleting the cluster!\n",
      "We are still working on deleting the cluster!\n",
      "We are still working on deleting the cluster!\n",
      "We are still working on deleting the cluster!\n",
      "We are still working on deleting the cluster!\n",
      "We are still working on deleting the cluster!\n",
      "We are still working on deleting the cluster!\n",
      "Cluster is dropped, please go ahead!\n",
      "Role is dropped, please go ahead!\n"
     ]
    }
   ],
   "source": [
    "#### Run to delete the created resources\n",
    "%run -i 'Redshift_Drop.py'\n",
    "# This will take 5-10 min\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data dictionary \n",
    "Every column is coming from the raw data except those unique IDs. For data layout, please see Data Layout.txt file in workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Project Write Up\n",
    "##### Rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "In this project, I choose AWS S3, Redshift, Spark as my tools since these are really good tools to handle big data in cloud enviroment.\n",
    "\n",
    "AWS S3: a well-known file system, which supports a lot of file type and provides great enough API that we can easily meet our file transfer requirement.\n",
    "\n",
    "AWS Redshift: a convenient tool, which is built on massive parallel processing data warehouse. It has different types of database and can handle large scale data sets. The most important benefit is its sscalability.  \n",
    "\n",
    "PySpark: easy to learn and implement and provides simple and comprehensive API. It also comes with a wide range of libraries like numpy, pandas, scikit-learn, seaborn, matplotlib etc.\n",
    "It is backed up by a huge and active community.\n",
    "\n",
    "#### Below are some scenarios that we might encounter in the future.\n",
    "##### The data was increased by 100x.\n",
    "\n",
    "If the data was increased by 100x, we will have 2 ways to deal with it.\n",
    "1.Ungrade the AWS Redshift and EC2 so we will more compute power. This is easier way, however, it will cost more.\n",
    "2.Partition the data with Loan Number and process eahc chunk in parallel.\n",
    "\n",
    "##### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "\n",
    "We can use airflow to create a dag to take care of the scheduling. This is also a further step I want to work on for this project.\n",
    "\n",
    "##### The database needed to be accessed by 100+ people.\n",
    "\n",
    "We can create different users and roles in AWS IAM and assign access to different user different roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
